{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNmSlReghWz0wAzf7XonFi5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vadhri/hpc-notebook/blob/main/mpi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gIyvsgYKPdm",
        "outputId": "2f4fc0ba-1a5b-4318-ebff-f8702c2081a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mpi4py in /usr/local/lib/python3.11/dist-packages (4.0.3)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "openmpi-bin is already the newest version (4.1.2-2ubuntu1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 29 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!pip install mpi4py\n",
        "!apt-get install -y openmpi-bin\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mpiexec --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3eevDRoWr5e",
        "outputId": "f8e902de-0806-4b07-8f00-7a44fce21051"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mpiexec (OpenRTE) 4.1.2\n",
            "\n",
            "Report bugs to http://www.open-mpi.org/community/help/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile hello.py\n",
        "\n",
        "from mpi4py import MPI\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "procs = comm.Get_size()\n",
        "rank = comm.Get_rank()\n",
        "if rank == 0:\n",
        "  print(f\"Number of processes: {procs}\") ## will be printed by each process.(or each core / machine.)\n",
        "\n",
        "print(f\"Message from processes: {procs} Rank : {rank}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9GH3tSXZSgI",
        "outputId": "c3f02bd6-f99d-458f-fbe8-6d434ee21292"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing hello.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo mpirun  --allow-run-as-root --use-hwthread-cpus -np 2 python hello.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-hrOaSIZwkt",
        "outputId": "63631a4a-c5f8-4f4c-ccad-6b85a9c8c93b"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of processes: 2\n",
            "Message from processes: 2 Rank : 1\n",
            "Message from processes: 2 Rank : 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!lscpu | grep 'CPU(s)'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7eEzgAYav4v",
        "outputId": "0e70806e-1e57-4d7b-8a7a-97143c4d5512"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU(s):                               2\n",
            "On-line CPU(s) list:                  0,1\n",
            "NUMA node0 CPU(s):                    0,1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"localhost slots=4\" > my_hostfile\n",
        "!sudo mpirun  --allow-run-as-root --hostfile my_hostfile -np 4 python send_recv.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhOlvmT3pZhK",
        "outputId": "309359ba-2b52-4d56-daff-38a79bd192b8"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Process 0 sent: Hello from Process 0\n",
            "Process 1 received: Hello from Process 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Single send-recv"
      ],
      "metadata": {
        "id": "LHuu_FNpqZb3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile send_recv.py\n",
        "\n",
        "from mpi4py import MPI\n",
        "\n",
        "comm = MPI.COMM_WORLD  # Get the communicator\n",
        "rank = comm.Get_rank()  # Get the rank of the current process\n",
        "\n",
        "if rank == 0:\n",
        "    data = \"Hello from Process 0\"\n",
        "    comm.send(data, dest=1)  # Send data to process 1\n",
        "    print(f\"Process {rank} sent: {data}\")\n",
        "\n",
        "elif rank == 1:\n",
        "    received_data = comm.recv(source=0)  # Receive data from process 0\n",
        "    print(f\"Process {rank} received: {received_data}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9tWwhdSpkil",
        "outputId": "3ebec4e5-9675-4664-95ea-0dca116b70f2"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting send_recv.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo mpirun  --allow-run-as-root --hostfile my_hostfile -np 4 python send_recv.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQ67V_2cp4Ci",
        "outputId": "b0745217-2343-4d1f-bd73-7b97f75e76e8"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Process 0 sent: Hello from Process 0\n",
            "Process 1 received: Hello from Process 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bidirectional send-receive"
      ],
      "metadata": {
        "id": "Z5LJe_qVq3-0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile send_recv_bidirectional.py\n",
        "from mpi4py import MPI\n",
        "\n",
        "comm = MPI.COMM_WORLD  # Get the communicator\n",
        "rank = comm.Get_rank()  # Get the rank of the current process\n",
        "\n",
        "if rank == 0:\n",
        "    data_to_send = \"Hello from Process 0\"\n",
        "    comm.send(data_to_send, dest=1)  # Send to Process 1\n",
        "    received_data = comm.recv(source=1)  # Receive from Process 1\n",
        "    print(f\"Process {rank} sent: {data_to_send}\")\n",
        "    print(f\"Process {rank} received: {received_data}\")\n",
        "\n",
        "elif rank == 1:\n",
        "    data_to_send = \"Hello from Process 1\"\n",
        "    received_data = comm.recv(source=0)  # Receive from Process 0\n",
        "    comm.send(data_to_send, dest=0)  # Send to Process 0\n",
        "    print(f\"Process {rank} received: {received_data}\")\n",
        "    print(f\"Process {rank} sent: {data_to_send}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWyYWUH-p_zt",
        "outputId": "9d006252-fd04-4363-e036-289a2970b1d9"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting send_recv_bidirectional.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo mpirun  --allow-run-as-root --hostfile my_hostfile -np 4 python send_recv_bidirectional.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AR3WkcEJqiG1",
        "outputId": "363c44a4-acef-401c-c6fb-a1a1c5316351"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Process 1 received: Hello from Process 0\n",
            "Process 1 sent: Hello from Process 1\n",
            "Process 0 sent: Hello from Process 0\n",
            "Process 0 received: Hello from Process 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile bidirec_square_numbers.py\n",
        "from mpi4py import MPI\n",
        "import time\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "rank = comm.Get_rank()\n",
        "\n",
        "num_iterations = 5  # Number of times we send/receive numbers\n",
        "\n",
        "if rank == 0:  # Sender process\n",
        "    for i in range(1, num_iterations + 1):\n",
        "        print(f\"Process {rank} sending: {i}\")\n",
        "        comm.send(i, dest=1)  # Send number to Process 1\n",
        "        squared_value = comm.recv(source=1)  # Receive squared value\n",
        "        print(f\"Process {rank} received squared: {squared_value}\")\n",
        "        time.sleep(1)  # Just to simulate processing delay\n",
        "\n",
        "elif rank == 1:  # Receiver process\n",
        "    for _ in range(num_iterations):\n",
        "        received_number = comm.recv(source=0)  # Receive number from Process 0\n",
        "        squared_number = received_number ** 2  # Square the number\n",
        "        comm.send(squared_number, dest=0)  # Send squared number back\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHiP2uGBqmfS",
        "outputId": "eb4fdb82-d281-48c0-fe0f-91f81ccda582"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting bidirec_square_numbers.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo mpirun  --allow-run-as-root --hostfile my_hostfile -np 4 python bidirec_square_numbers.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9eVt7RJyq_n-",
        "outputId": "ea380d03-b69d-4dad-ac94-c199082142bf"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Process 0 sending: 1\n",
            "Process 0 received squared: 1\n",
            "Process 0 sending: 2\n",
            "Process 0 received squared: 4\n",
            "Process 0 sending: 3\n",
            "Process 0 received squared: 9\n",
            "Process 0 sending: 4\n",
            "Process 0 received squared: 16\n",
            "Process 0 sending: 5\n",
            "Process 0 received squared: 25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Process sync with shared memory"
      ],
      "metadata": {
        "id": "YeX4xsX_rqPV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile bidirec_shared_memory.py\n",
        "\n",
        "from mpi4py import MPI\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "rank = comm.Get_rank()\n",
        "size = comm.Get_size()\n",
        "\n",
        "N = 5  # Number of iterations\n",
        "array_size = 4  # Size of shared array\n",
        "\n",
        "# Create a shared memory buffer\n",
        "win = MPI.Win.Allocate_shared(array_size * np.dtype('i').itemsize, np.dtype('i').itemsize, comm=comm)\n",
        "\n",
        "# Get shared memory array reference\n",
        "buf, itemsize = win.Shared_query(0)  # Rank 0 creates the memory\n",
        "shared_array = np.ndarray(buffer=buf, dtype='i', shape=(array_size,))\n",
        "\n",
        "# Create a flag in shared memory\n",
        "flag_win = MPI.Win.Allocate_shared(np.dtype('i').itemsize, np.dtype('i').itemsize, comm=comm)\n",
        "flag_buf, _ = flag_win.Shared_query(0)\n",
        "flag = np.ndarray(buffer=flag_buf, dtype='i', shape=(1,))\n",
        "\n",
        "if rank == 0:\n",
        "    for iter in range(N):\n",
        "        # Initialize array\n",
        "        shared_array[:] = np.arange(1, array_size + 1) * (iter + 1)\n",
        "        print(f\"Process 0: Initialized array: {shared_array}\")\n",
        "\n",
        "        # Signal process 1 to start processing\n",
        "        flag[0] = 1\n",
        "        flag_win.Sync()\n",
        "\n",
        "        # Wait for process 1 to finish\n",
        "        while flag[0] != 2:\n",
        "            flag_win.Sync()\n",
        "            time.sleep(0.01)\n",
        "\n",
        "        # Sum the modified array\n",
        "        total = np.sum(shared_array)\n",
        "        print(f\"Process 0: Sum after modification: {total}\")\n",
        "\n",
        "        # Reset flag\n",
        "        flag[0] = 0\n",
        "        flag_win.Sync()\n",
        "\n",
        "elif rank == 1:\n",
        "    for _ in range(N):\n",
        "        # Wait for signal from process 0\n",
        "        while flag[0] != 1:\n",
        "            flag_win.Sync()\n",
        "            time.sleep(0.01)\n",
        "\n",
        "        # Multiply values by 2\n",
        "        shared_array[:] *= 2\n",
        "        print(f\"Process 1: Modified array: {shared_array}\")\n",
        "\n",
        "        # Signal process 0 that modification is done\n",
        "        flag[0] = 2\n",
        "        flag_win.Sync()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVpz97lvrEvd",
        "outputId": "f2fad42a-1021-40c2-f353-ab35bd9d9f75"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting bidirec_shared_memory.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "07c9v7or5iir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo mpirun  --allow-run-as-root --hostfile my_hostfile -np 4 python bidirec_shared_memory.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtTNWC_qrt2a",
        "outputId": "5979aa0d-d42a-4f91-88ae-d4b4b2fb4b19"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Process 0: Initialized array: [1 2 3 4]\n",
            "Process 1: Modified array: [2 4 6 8]\n",
            "Process 0: Sum after modification: 20\n",
            "Process 0: Initialized array: [2 4 6 8]\n",
            "Process 1: Modified array: [ 4  8 12 16]\n",
            "Process 0: Sum after modification: 40\n",
            "Process 0: Initialized array: [ 3  6  9 12]\n",
            "Process 1: Modified array: [ 6 12 18 24]\n",
            "Process 0: Sum after modification: 60\n",
            "Process 0: Initialized array: [ 4  8 12 16]\n",
            "Process 1: Modified array: [ 8 16 24 32]\n",
            "Process 0: Sum after modification: 80\n",
            "Process 0: Initialized array: [ 5 10 15 20]\n",
            "Process 1: Modified array: [10 20 30 40]\n",
            "Process 0: Sum after modification: 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### isend and irecv with tagging"
      ],
      "metadata": {
        "id": "5VmD0ZGb5laH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile non-blocking-bidir-sendrecv.py\n",
        "from mpi4py import MPI\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "rank = comm.Get_rank()\n",
        "\n",
        "n_iterations = 10  # Number of iterations\n",
        "\n",
        "if rank == 0:\n",
        "    for i in range(n_iterations):\n",
        "        value = i\n",
        "        print(f\"Iteration {i+1}: Process {rank} sending {value} to Process 1\")\n",
        "        req = comm.isend(value, dest=1, tag=11)\n",
        "        req.wait()\n",
        "\n",
        "        # Receive the computed value from Process 1\n",
        "        req = comm.irecv(source=1, tag=22)\n",
        "        value = req.wait()\n",
        "        print(f\"Iteration {i+1}: Process {rank} received {value} from Process 1\")\n",
        "\n",
        "elif rank == 1:\n",
        "    for i in range(n_iterations):\n",
        "        # Receive value from Process 0\n",
        "        req = comm.irecv(source=0, tag=11)\n",
        "        value = req.wait()\n",
        "        print(f\"Iteration {i+1}: Process {rank} received {value} from Process 0\")\n",
        "\n",
        "        # Perform computation (e.g., square the value)\n",
        "        computed_value = 3.14*(value ** 2)\n",
        "        print(f\"Iteration {i+1}: Process {rank} computed {computed_value}\")\n",
        "\n",
        "        # Send computed value back to Process 0\n",
        "        req = comm.isend(computed_value, dest=0, tag=22)\n",
        "        req.wait()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EC74kGBosBBy",
        "outputId": "506f106b-8dbd-4847-f7ea-c392b19d195f"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting non-blocking-bidir-sendrecv.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo mpirun  --allow-run-as-root --hostfile my_hostfile -np 4 python non-blocking-bidir-sendrecv.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CeOKYaF0we1E",
        "outputId": "a59a1baa-a920-4cc3-a8d0-61ea27851645"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: Process 0 sending 0 to Process 1\n",
            "Iteration 1: Process 1 received 0 from Process 0\n",
            "Iteration 1: Process 1 computed 0.0\n",
            "Iteration 1: Process 0 received 0.0 from Process 1\n",
            "Iteration 2: Process 0 sending 1 to Process 1\n",
            "Iteration 2: Process 1 received 1 from Process 0\n",
            "Iteration 2: Process 1 computed 3.14\n",
            "Iteration 2: Process 0 received 3.14 from Process 1\n",
            "Iteration 3: Process 0 sending 2 to Process 1\n",
            "Iteration 3: Process 1 received 2 from Process 0\n",
            "Iteration 3: Process 1 computed 12.56\n",
            "Iteration 3: Process 0 received 12.56 from Process 1\n",
            "Iteration 4: Process 0 sending 3 to Process 1\n",
            "Iteration 4: Process 1 received 3 from Process 0\n",
            "Iteration 4: Process 1 computed 28.26\n",
            "Iteration 4: Process 0 received 28.26 from Process 1\n",
            "Iteration 5: Process 0 sending 4 to Process 1\n",
            "Iteration 5: Process 1 received 4 from Process 0\n",
            "Iteration 5: Process 1 computed 50.24\n",
            "Iteration 5: Process 0 received 50.24 from Process 1\n",
            "Iteration 6: Process 0 sending 5 to Process 1\n",
            "Iteration 6: Process 1 received 5 from Process 0\n",
            "Iteration 6: Process 1 computed 78.5\n",
            "Iteration 6: Process 0 received 78.5 from Process 1\n",
            "Iteration 7: Process 0 sending 6 to Process 1\n",
            "Iteration 7: Process 1 received 6 from Process 0\n",
            "Iteration 7: Process 1 computed 113.04\n",
            "Iteration 7: Process 0 received 113.04 from Process 1\n",
            "Iteration 8: Process 0 sending 7 to Process 1\n",
            "Iteration 8: Process 1 received 7 from Process 0\n",
            "Iteration 8: Process 1 computed 153.86\n",
            "Iteration 8: Process 0 received 153.86 from Process 1\n",
            "Iteration 9: Process 0 sending 8 to Process 1\n",
            "Iteration 9: Process 1 received 8 from Process 0\n",
            "Iteration 9: Process 1 computed 200.96\n",
            "Iteration 9: Process 0 received 200.96 from Process 1\n",
            "Iteration 10: Process 0 sending 9 to Process 1\n",
            "Iteration 10: Process 1 received 9 from Process 0\n",
            "Iteration 10: Process 1 computed 254.34\n",
            "Iteration 10: Process 0 received 254.34 from Process 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### broadcasting"
      ],
      "metadata": {
        "id": "X_Slg5ac5x2x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile non-blocking-broadcast-sendrecv.py\n",
        "from mpi4py import MPI\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "rank = comm.Get_rank()\n",
        "\n",
        "n_iterations = 10  # Number of iterations\n",
        "\n",
        "for i in range(n_iterations):\n",
        "    if rank == 0:\n",
        "        value = i\n",
        "        print(f\"Iteration {i+1}: Process {rank} broadcasting {value}\")\n",
        "    else:\n",
        "        value = None  # Other processes start with empty value\n",
        "\n",
        "    # Rank 0 broadcasts the value to all ranks\n",
        "    value = comm.bcast(value, root=0)\n",
        "\n",
        "    if rank == 1:\n",
        "        print(f\"Iteration {i+1}: Process {rank} received {value} from Broadcast\")\n",
        "\n",
        "        # Perform computation (e.g., square the value and multiply by 3.14)\n",
        "        computed_value = 3.14 * (value ** 2)\n",
        "        print(f\"Iteration {i+1}: Process {rank} computed {computed_value}\")\n",
        "\n",
        "        # Send computed value back to Process 0\n",
        "        req = comm.isend(computed_value, dest=0, tag=22)\n",
        "        req.wait()\n",
        "\n",
        "    elif rank == 0:\n",
        "        # Receive the computed value from Process 1\n",
        "        req = comm.irecv(source=1, tag=22)\n",
        "        result = req.wait()\n",
        "        print(f\"Iteration {i+1}: Process {rank} received {result} from Process 1\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RwVQUB_Ywhj4",
        "outputId": "fcf3052a-5cb3-40f7-b624-17e07993aabe"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting non-blocking-broadcast-sendrecv.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo mpirun  --allow-run-as-root --hostfile my_hostfile -np 4 python non-blocking-broadcast-sendrecv.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KBQLgZl71GC",
        "outputId": "2e9790a4-3336-479f-90c3-d04f81defcd7"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: Process 0 broadcasting 0\n",
            "Iteration 1: Process 1 received 0 from Broadcast\n",
            "Iteration 1: Process 1 computed 0.0\n",
            "Iteration 1: Process 0 received 0.0 from Process 1\n",
            "Iteration 2: Process 0 broadcasting 1\n",
            "Iteration 2: Process 1 received 1 from Broadcast\n",
            "Iteration 2: Process 1 computed 3.14\n",
            "Iteration 2: Process 0 received 3.14 from Process 1\n",
            "Iteration 3: Process 0 broadcasting 2\n",
            "Iteration 3: Process 1 received 2 from Broadcast\n",
            "Iteration 3: Process 1 computed 12.56\n",
            "Iteration 3: Process 0 received 12.56 from Process 1\n",
            "Iteration 4: Process 0 broadcasting 3\n",
            "Iteration 4: Process 1 received 3 from Broadcast\n",
            "Iteration 4: Process 1 computed 28.26\n",
            "Iteration 4: Process 0 received 28.26 from Process 1\n",
            "Iteration 5: Process 0 broadcasting 4\n",
            "Iteration 5: Process 1 received 4 from Broadcast\n",
            "Iteration 5: Process 1 computed 50.24\n",
            "Iteration 5: Process 0 received 50.24 from Process 1\n",
            "Iteration 6: Process 0 broadcasting 5\n",
            "Iteration 6: Process 1 received 5 from Broadcast\n",
            "Iteration 6: Process 1 computed 78.5\n",
            "Iteration 6: Process 0 received 78.5 from Process 1\n",
            "Iteration 7: Process 0 broadcasting 6\n",
            "Iteration 7: Process 1 received 6 from Broadcast\n",
            "Iteration 7: Process 1 computed 113.04\n",
            "Iteration 7: Process 0 received 113.04 from Process 1\n",
            "Iteration 8: Process 0 broadcasting 7\n",
            "Iteration 8: Process 1 received 7 from Broadcast\n",
            "Iteration 8: Process 1 computed 153.86\n",
            "Iteration 8: Process 0 received 153.86 from Process 1\n",
            "Iteration 9: Process 0 broadcasting 8\n",
            "Iteration 9: Process 1 received 8 from Broadcast\n",
            "Iteration 9: Process 1 computed 200.96\n",
            "Iteration 9: Process 0 received 200.96 from Process 1\n",
            "Iteration 10: Process 0 broadcasting 9\n",
            "Iteration 10: Process 1 received 9 from Broadcast\n",
            "Iteration 10: Process 1 computed 254.34\n",
            "Iteration 10: Process 0 received 254.34 from Process 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scatter an array between processes"
      ],
      "metadata": {
        "id": "5qdCOEcQ-hA1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile non-blocking-scatter-sendrecv.py\n",
        "from mpi4py import MPI\n",
        "import numpy as np\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "rank = comm.Get_rank()\n",
        "size = comm.Get_size()  # Get number of processes\n",
        "\n",
        "# Root process prepares data\n",
        "if rank == 0:\n",
        "    array_to_scatter = np.arange(size * 25)  # Total size should be divisible by number of processes\n",
        "    chunks = np.array_split(array_to_scatter, size)  # Split data into `size` parts\n",
        "else:\n",
        "    chunks = None\n",
        "\n",
        "# Each process receives its chunk\n",
        "recv_data = comm.scatter(chunks, root=0)\n",
        "\n",
        "print(f\"Process {rank} received: {recv_data}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obeNToEO7268",
        "outputId": "106d0ff7-d478-4efa-c65d-e2126129ca3e"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting non-blocking-scatter-sendrecv.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo mpirun  --allow-run-as-root --hostfile my_hostfile -np 4 python non-blocking-scatter-sendrecv.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmfE6_hj-ztF",
        "outputId": "3b3657e5-bd93-4a29-a503-089c860e26b7"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Process 1 received: [25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n",
            " 49]Process 2 received: [50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73\n",
            " 74]\n",
            "\n",
            "Process 3 received: [75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98\n",
            " 99]\n",
            "Process 0 received: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
            " 24]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scatter uneven size of data."
      ],
      "metadata": {
        "id": "bhx89Fd1gv_c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile uneven-scatter.py\n",
        "from mpi4py import MPI\n",
        "import numpy as np\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "rank = comm.Get_rank()\n",
        "size = comm.Get_size()\n",
        "\n",
        "# Root process prepares the data\n",
        "if rank == 0:\n",
        "    array_to_scatter = np.arange(103, dtype=np.int32)  # Explicit int32\n",
        "    split_sizes = np.array_split(array_to_scatter, size)\n",
        "    counts = np.array([len(chunk) for chunk in split_sizes], dtype=np.int32)\n",
        "    displacements = np.insert(np.cumsum(counts), 0, 0)[:-1].astype(np.int32)\n",
        "    flat_data = np.concatenate(split_sizes)\n",
        "    print(f\"Rank 0: Counts = {counts}, Displacements = {displacements}\")\n",
        "    print(f\"Rank 0: Flat data = {flat_data}\")\n",
        "else:\n",
        "    counts = None\n",
        "    displacements = None\n",
        "    flat_data = None\n",
        "\n",
        "# Broadcast counts and displacements\n",
        "counts = comm.bcast(counts, root=0)\n",
        "displacements = comm.bcast(displacements, root=0)\n",
        "\n",
        "# Allocate receive buffer with explicit type\n",
        "recv_data = np.zeros(counts[rank], dtype=np.int32)\n",
        "\n",
        "# Explicit send buffer\n",
        "if rank == 0:\n",
        "    sendbuf = [flat_data, counts, displacements, MPI.INT]\n",
        "else:\n",
        "    sendbuf = None\n",
        "\n",
        "# Scatter the data\n",
        "comm.Scatterv(sendbuf, recv_data, root=0)\n",
        "\n",
        "print(f\"Process {rank} expected {counts[rank]} elements, received: {recv_data}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t60ThxYQ-4ij",
        "outputId": "676953be-231c-43e0-b926-43fa8b1cf94f"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting uneven-scatter.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo mpirun  --allow-run-as-root --hostfile my_hostfile -np 4 python uneven-scatter.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mD0gR9CcFjbv",
        "outputId": "3bf65af7-e988-4cb7-99ad-d3db163fc1eb"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rank 0: Counts = [26 26 26 25], Displacements = [ 0 26 52 78]\n",
            "Rank 0: Flat data = [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
            "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
            "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
            "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
            "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
            "  90  91  92  93  94  95  96  97  98  99 100 101 102]\n",
            "Process 0 expected 26 elements, received: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
            " 24 25]Process 2 expected 26 elements, received: [52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75\n",
            " 76 77]\n",
            "\n",
            "Process 1 expected 26 elements, received: [26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49\n",
            " 50 51]\n",
            "Process 3 expected 25 elements, received: [ 78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95\n",
            "  96  97  98  99 100 101 102]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nU4iIC_rFlrD"
      },
      "execution_count": 147,
      "outputs": []
    }
  ]
}